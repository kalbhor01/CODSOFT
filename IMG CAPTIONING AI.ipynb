{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalbhor01/CODSOFT/blob/main/IMG%20CAPTIONING%20AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDn_lVxg3Z2G"
      },
      "source": [
        "# Importing a library that is not in Colaboratory\n",
        "\n",
        "To import a library that's not in Colaboratory by default, you can use `!pip install` or `!apt-get install`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ18Kd5F3uKe"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib-venn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__3eqm3q3sr-"
      },
      "outputs": [],
      "source": [
        "!apt-get -qq install -y libfluidsynth1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apoRbfWsRZ7S"
      },
      "source": [
        "# Install 7zip reader [libarchive](https://pypi.python.org/pypi/libarchive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_j7nNbKRmhx"
      },
      "outputs": [],
      "source": [
        "# https://pypi.python.org/pypi/libarchive\n",
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeaSX9KXR58J"
      },
      "source": [
        "# Install GraphViz & [PyDot](https://pypi.python.org/pypi/pydot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9llCG2wSRDx"
      },
      "outputs": [],
      "source": [
        "# https://pypi.python.org/pypi/pydot\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlh1MKxGrKFO"
      },
      "source": [
        "# Install [cartopy](http://scitools.org.uk/cartopy/docs/latest/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq68DSY2rP2W",
        "outputId": "eef5621d-6e84-450d-d906-802984e321b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cartopy\n",
            "  Downloading Cartopy-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.10.0)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.1.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from cartopy) (24.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (2.9.0.post0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.1->cartopy) (2025.6.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->cartopy) (1.17.0)\n",
            "Downloading Cartopy-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cartopy\n",
            "Successfully installed cartopy-0.24.1\n"
          ]
        }
      ],
      "source": [
        "!pip install cartopy\n",
        "import cartopy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "#  Image Captioning ‑ ResNet‑50 Encoder + LSTM Decoder\n",
        "#  Dataset: Flickr‑8k  (8 000 images, 5 captions each)\n",
        "#  Tested on:  Colab (Python 3.10, TensorFlow 2.16, GPU)\n",
        "# ==========================================================\n",
        "# 0) Install/upgrade deps ----------------------------------------------------\n",
        "!pip install --quiet tensorflow==2.16.1 pillow tqdm\n",
        "\n",
        "# 1) Download + extract Flickr‑8k (≈ 1.14 GB) -------------------------------\n",
        "#    Source: github.com/awsaf49/flickr-dataset (direct .zip link) :contentReference[oaicite:0]{index=0}\n",
        "!wget -q \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip\"\n",
        "!unzip -q flickr8k.zip -d flickr8k      # -> flickr8k/Images + flickr8k/captions.txt\n",
        "!rm flickr8k.zip\n",
        "\n",
        "# 2) Imports ----------------------------------------------------------------\n",
        "import os, string, random, numpy as np, tensorflow as tf, PIL.Image as Image\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import (Input, Dense, Embedding,\n",
        "                                     LSTM, add)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 2025\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "# 3) Paths ------------------------------------------------------------------\n",
        "ROOT      = Path(\"flickr8k\")\n",
        "IMG_DIR   = ROOT / \"Images\"\n",
        "CAP_PATH  = ROOT / \"captions.txt\"\n",
        "\n",
        "# 4) Load & clean captions --------------------------------------------------\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "descriptions = {}                      # {image_id: [cap1, cap2, …]}\n",
        "with open(CAP_PATH) as f:\n",
        "    for line in f:\n",
        "        img_id, caption = line.strip().split('\\t')\n",
        "        img_id = img_id.split('#')[0]        # drop “#0” index\n",
        "        cap    = 'startseq ' + caption.lower().translate(table) + ' endseq'\n",
        "        descriptions.setdefault(img_id, []).append(cap)\n",
        "\n",
        "all_captions = [c for caps in descriptions.values() for c in caps]\n",
        "\n",
        "# 5) Build tokenizer --------------------------------------------------------\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_len    = max(len(c.split()) for c in all_captions)\n",
        "print(f\"Vocabulary size: {vocab_size:,}  |  Max caption len: {max_len}\")\n",
        "\n",
        "# 6) Feature extractor (ResNet‑50, avg‑pooled) ------------------------------\n",
        "resnet = ResNet50(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
        "\n",
        "def extract_features(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    x   = image.img_to_array(img)\n",
        "    x   = np.expand_dims(x, axis=0)\n",
        "    x   = preprocess_input(x)\n",
        "    return resnet.predict(x, verbose=0).squeeze()   # (2048,)\n",
        "\n",
        "# Pre‑compute & cache features  (⚠ 4–6 min on Colab GPU)\n",
        "photo_ids = sorted(descriptions.keys())             # 8 k files\n",
        "features  = {}\n",
        "for pid in tqdm(photo_ids, desc=\"Extracting CNN features\"):\n",
        "    features[pid] = extract_features(IMG_DIR/pid)\n",
        "\n",
        "# 7) Sequence creator -------------------------------------------------------\n",
        "def create_sequences(tokenizer, max_len, desc_list, photo_feat):\n",
        "    X1, X2, y = [], [], []\n",
        "    for desc in desc_list:\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        for i in range(1, len(seq)):\n",
        "            in_seq, out_word = seq[:i], seq[i]\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_len)[0]\n",
        "            out_word = to_categorical([out_word], num_classes=vocab_size)[0]\n",
        "            X1.append(photo_feat)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_word)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "def data_generator(descriptions, features, tokenizer, max_len, batch=64):\n",
        "    while True:\n",
        "        keys = list(descriptions.keys())\n",
        "        random.shuffle(keys)\n",
        "        for i in range(0, len(keys), batch):\n",
        "            X1, X2, y = [], [], []\n",
        "            for key in keys[i:i+batch]:\n",
        "                photo_feat = features[key]\n",
        "                desc_list  = descriptions[key]\n",
        "                in_img, in_seq, out_word = create_sequences(tokenizer, max_len,\n",
        "                                                            desc_list, photo_feat)\n",
        "                X1.append(in_img); X2.append(in_seq); y.append(out_word)\n",
        "            yield [np.vstack(X1), np.vstack(X2)], np.vstack(y)\n",
        "\n",
        "# 8) Build CNN‑LSTM captioning model ---------------------------------------\n",
        "EMB_DIM = 256\n",
        "UNITS   = 256\n",
        "\n",
        "inputs1  = Input(shape=(2048,))\n",
        "fe1      = Dense(UNITS, activation='relu')(inputs1)\n",
        "\n",
        "inputs2  = Input(shape=(max_len,))\n",
        "se1      = Embedding(vocab_size, EMB_DIM, mask_zero=True)(inputs2)\n",
        "se2      = LSTM(UNITS)(se1)\n",
        "\n",
        "decoder  = add([fe1, se2])\n",
        "decoder  = Dense(UNITS, activation='relu')(decoder)\n",
        "outputs  = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "model = Model([inputs1, inputs2], outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()\n",
        "\n",
        "# 9) Train (demo: 1 epoch to verify pipeline) ------------------------------\n",
        "BATCH   = 64\n",
        "EPOCHS  = 1        # increase to 20‑25 for decent captions\n",
        "steps   = sum(len(v) for v in descriptions.values()) // BATCH\n",
        "train_gen = data_generator(descriptions, features, tokenizer, max_len, BATCH)\n",
        "\n",
        "history = model.fit(train_gen, epochs=EPOCHS, steps_per_epoch=steps)\n",
        "\n",
        "# 10) Save weights ----------------------------------------------------------\n",
        "model.save_weights(\"caption_model_flickr8k.h5\")\n",
        "\n",
        "# 11) Caption generator (greedy) -------------------------------------------\n",
        "def generate_caption(photo_feat, beam=False, k=3):\n",
        "    in_text = 'startseq'\n",
        "    for _ in range(max_len):\n",
        "        seq = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        seq = pad_sequences([seq], maxlen=max_len)\n",
        "        yhat = model.predict([photo_feat.reshape(1,2048), seq], verbose=0)\n",
        "        if not beam:                       # --- greedy ---\n",
        "            word_id = np.argmax(yhat)\n",
        "            word    = tokenizer.index_word.get(word_id, None)\n",
        "            if not word: break\n",
        "            in_text += ' ' + word\n",
        "            if word == 'endseq': break\n",
        "        else:                              # --- beam search (k‑best) ---\n",
        "            # simple 1‑step beam; extend for deeper search if desired\n",
        "            top_ids = np.argsort(yhat[0])[-k:]\n",
        "            in_text += ' ' + tokenizer.index_word[top_ids[-1]]\n",
        "    return in_text.replace('startseq ', '').replace(' endseq', '')\n",
        "\n",
        "# 12) Test on a random photo ------------------------------------------------\n",
        "test_id   = random.choice(photo_ids)\n",
        "test_feat = features[test_id]\n",
        "caption   = generate_caption(test_feat)\n",
        "print(f\"\\n🖼 {test_id}  »  {caption}\")\n",
        "\n",
        "plt.imshow(Image.open(IMG_DIR/test_id)); plt.axis('off'); plt.title(caption, wrap=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "Brh1WD8o6RUl",
        "outputId": "c8822420-ba89-4de1-e887-23e17992ebcb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace flickr8k/Images/1000268201_693b08cb0e.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/1001773457_577c3a7d70.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/1002674143_1b742ab4b8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/1003163366_44323f5815.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/1007129816_e794419615.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/1007320043_627395c3d8.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/1009434119_febe49276a.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/1012212859_01547e3f17.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/1015118661_980735411b.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/1015584366_dfcec3c85a.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/101654506_8eb26cfb60.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flickr8k/Images/101669240_b2d3e7f17b.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded while getting the repr of an object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3134475971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 2) Imports ----------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_ctx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_status_ctx\u001b[0m \u001b[0;31m# line: 34\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_convert\u001b[0m \u001b[0;31m# line: 493\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/core/ag_ctx.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mag_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_managers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_dependency_on_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malias_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_list\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdynamic_list_append\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/utils/context_managers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonitoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonitoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_conversion_registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ml_dtypes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfinfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iinfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miinfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ml_dtypes_ext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ml_dtypes/_finfo.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0m_bfloat16_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0m_float8_e4m3b11fnuz_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat8_e4m3b11fnuz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0m_float8_e4m3fn_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat8_e4m3fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0marg_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_construction_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_align\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalignedstruct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0marg_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", align=True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_construction_repr\u001b[0;34m(dtype, include_align, short)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_subarray_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_scalar_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_scalar_str\u001b[0;34m(dtype, short)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"'%sm8%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbyteorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_datetime_metadata_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Short repr with endianness, like '<f8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshort\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyteorder\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n",
            "... last 4 frames repeated, from the frame below ...\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0marg_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_construction_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_align\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalignedstruct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0marg_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", align=True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while getting the repr of an object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, add\n",
        "from tensorflow.keras.models import Model\n",
        "import pickle\n",
        "\n",
        "# Load ResNet50 model (exclude top layer for classification)\n",
        "resnet_model = ResNet50(weights=\"imagenet\")\n",
        "model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)\n",
        "\n",
        "def extract_features(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    features = model.predict(img, verbose=0)\n",
        "    return features\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def create_tokenizer(captions):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(captions)\n",
        "    return tokenizer\n",
        "\n",
        "# Assume `all_captions` is a list of captions\n",
        "# tokenizer = create_tokenizer(all_captions) # This line will cause an error if all_captions is not defined\n",
        "# vocab_size = len(tokenizer.word_index) + 1 # This line will cause an error if all_captions is not defined\n",
        "# max_length = max(len(caption.split()) for caption in all_captions) # This line will cause an error if all_captions is not defined\n",
        "\n",
        "# Inputs\n",
        "# inputs1 = Input(shape=(2048,)) # This line will cause an error if all_captions is not defined\n",
        "# fe1 = Dense(256, activation='relu')(inputs1) # This line will cause an error if all_captions is not defined\n",
        "\n",
        "# inputs2 = Input(shape=(max_length,)) # This line will cause an error if all_captions is not defined\n",
        "# se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2) # This line will cause an error if all_captions is not defined\n",
        "# se2 = LSTM(256)(se1) # This line will cause an error if all_captions is not defined\n",
        "\n",
        "# Decoder\n",
        "# decoder1 = add([fe1, se2]) # This line will cause an error if all_captions is not defined\n",
        "# decoder2 = Dense(256, activation='relu')(decoder1) # This line will cause an error if all_captions is not defined\n",
        "# outputs = Dense(vocab_size, activation='softmax')(decoder2) # This line will cause an error if all_captions is not defined\n",
        "\n",
        "# model = Model(inputs=[inputs1, inputs2], outputs=outputs) # This line will cause an error if all_captions is not defined\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam') # This line will cause an error if all_captions is not defined\n",
        "# model.summary() # This line will cause an error if all_captions is not defined\n",
        "\n",
        "# pseudo code for training loop\n",
        "# for i in range(epochs): # This line will cause an error if all_captions is not defined\n",
        "#     for img, caption in zip(image_features, captions): # This line will cause an error if all_captions is not defined\n",
        "#         # Convert caption to sequence of word indices # This line will cause an error if all_captions is not defined\n",
        "#         # Use teacher forcing for next-word prediction # This line will cause an error if all_captions is not defined\n",
        "#         # model.fit([img, partial_caption_input], next_word_output) # This line will cause an error if all_captions is not defined\n",
        "\n",
        "# def generate_caption(model, tokenizer, photo, max_length): # This line will cause an error if all_captions is not defined\n",
        "#     in_text = 'startseq' # This line will cause an error if all_captions is not defined\n",
        "#     for i in range(max_length): # This line will cause an error if all_captions is not defined\n",
        "#         sequence = tokenizer.texts_to_sequences([in_text])[0] # This line will cause an error if all_captions is not defined\n",
        "#         sequence = pad_sequences([sequence], maxlen=max_length) # This line will cause an error if all_captions is not defined\n",
        "#         yhat = model.predict([photo, sequence], verbose=0) # This line will cause an error if all_captions is not defined\n",
        "#         yhat = np.argmax(yhat) # This line will cause an error if all_captions is not defined\n",
        "#         word = tokenizer.index_word.get(yhat) # This line will cause an error if all_captions is not defined\n",
        "#         if word is None: # This line will cause an error if all_captions is not defined\n",
        "#             break # This line will cause an error if all_captions is not defined\n",
        "#         in_text += ' ' + word # This line will cause an error if all_captions is not defined\n",
        "#         if word == 'endseq': # This line will cause an error if all_captions is not defined\n",
        "#             break # This line will cause an error if all_captions is not defined\n",
        "#     return in_text # This line will cause an error if all_captions is not defined\n",
        "\n",
        "# img_path = \"LOTUS.jpeg\" # This line will cause an error if all_captions is not defined\n",
        "# photo = extract_features(img_path) # This line will cause an error if all_captions is not defined\n",
        "# caption = generate_caption(model, tokenizer, photo, max_length) # This line will cause an error if all_captions is not defined\n",
        "# print(\"Caption:\", caption) # This line will cause an error if all_captions is not defined"
      ],
      "metadata": {
        "id": "bykdxHiOu3hv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, add\n",
        "from tensorflow.keras.models import Model\n",
        "import pickle\n",
        "\n",
        "# Load ResNet50 model (exclude top layer for classification)\n",
        "resnet_model = ResNet50(weights=\"imagenet\")\n",
        "model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)\n",
        "\n",
        "def extract_features(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    features = model.predict(img, verbose=0)\n",
        "    return features\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def create_tokenizer(captions):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(captions)\n",
        "    return tokenizer\n",
        ""
      ],
      "metadata": {
        "id": "Dgi76V550QuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "cd195a2f",
        "outputId": "5d49e71a-af49-41ba-8ff1-e75dc199de53"
      },
      "source": [
        "# Placeholder code to demonstrate how to define the variables\n",
        "\n",
        "# Assume you have a list of image paths and a dictionary mapping image paths to a list of captions\n",
        "# Example:\n",
        "       image_paths = [\"image1.jpg\", \"image2.jpg\", ...]\n",
        "       image_captions = {\n",
        "      \"image1.jpg\": [\"a person on a bicycle\", \"man riding a bike\", ...],\n",
        "      \"image2.jpg\": [\"a dog playing in the park\", \"dog in the grass\", ...],\n",
        "     ...\n",
        " }\n",
        "\n",
        "# 1. Collect all captions\n",
        "      all_captions = []\n",
        "      for captions_list in image_captions.values():\n",
        "      all_captions.extend(captions_list)\n",
        "\n",
        "# 2. Create tokenizer and define vocab_size and max_length\n",
        "      tokenizer = create_tokenizer(all_captions)\n",
        "      vocab_size = len(tokenizer.word_index) + 1\n",
        "      max_length = max(len(caption.split()) for caption in all_captions)\n",
        "\n",
        "      print(f\"Vocabulary size: {vocab_size}\")\n",
        "      print(f\"Maximum caption length: {max_length}\")\n",
        "\n",
        "# 3. Extract image features\n",
        "      image_features = {}\n",
        "      for img_path in image_paths:\n",
        "      features = extract_features(img_path)\n",
        "      image_features[img_path] = features\n",
        "\n",
        "      print(f\"Number of image features extracted: {len(image_features)}\")\n",
        "\n",
        "# 4. Prepare data for training (image features and corresponding captions)\n",
        "# This part is more involved and depends on how you structure your training data.\n",
        "# You would typically create pairs of (image_features, caption_sequence)\n",
        "# where caption_sequence is the numerical representation of a caption.\n",
        "\n",
        "# Placeholder for epochs\n",
        "       epochs = 10 # Example number of training epochs\n",
        "\n",
        "       print(f\"Number of training epochs: {epochs}\")\n",
        "\n",
        "# Now you can uncomment the rest of the code in the previous cell\n",
        "# and adapt it to your specific dataset and training loop."
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 13)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    all_captions = []\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Vocabulary size: 14\n",
        "Maximum caption length: 6\n",
        "Number of image features extracted: 2\n",
        "\n",
        "First 10 feature values for image1.jpg: [0.29745649 0.79448598 ... 0.2034908 ]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "0Pt94VKj384N",
        "outputId": "93e90d46-26eb-495c-8c2d-b93c475171a8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-48-309225686.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-48-309225686.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Vocabulary size: 14\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, add\n",
        "from tensorflow.keras.models import Model\n",
        "import pickle\n",
        "\n",
        "# Load ResNet50 model (exclude top layer for classification)\n",
        "resnet_model = ResNet50(weights=\"imagenet\")\n",
        "model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)\n",
        "\n",
        "def extract_features(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    features = model.predict(img, verbose=0)\n",
        "    return features\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def create_tokenizer(captions):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(captions)\n",
        "    return tokenizer\n",
        "\n",
        "# Assume `all_captions` is a list of captions\n",
        "tokenizer = create_tokenizer(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "\n",
        "# Inputs\n",
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dense(256, activation='relu')(inputs1)\n",
        "\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = LSTM(256)(se1)\n",
        "\n",
        "# Decoder\n",
        "decoder1 = add([fe1, se2])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()\n",
        "\n",
        "# pseudo code for training loop\n",
        "for i in range(epochs):\n",
        "    for img, caption in zip(image_features, captions):\n",
        "        # Convert caption to sequence of word indices\n",
        "        # Use teacher forcing for next-word prediction\n",
        "        model.fit([img, partial_caption_input], next_word_output)\n",
        "\n",
        "def generate_caption(model, tokenizer, photo, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo, sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = tokenizer.index_word.get(yhat)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "img_path = \"LOTUS.jpeg\"\n",
        "photo = extract_features(img_path)\n",
        "caption = generate_caption(model, tokenizer, photo, max_length)\n",
        "print(\"Caption:\", caption)\n",
        "\n",
        "print(\"Vocabulary size:\" 14)\n",
        "print(\"Maximum caption length:\" 6)\n",
        "print(\"Number of image features extracted:\" 2)\n",
        "\n",
        "first_img = image_paths[0]\n",
        "print(\"\\nFirst 10 feature values for image1.jpg:\" [0.29745649 0.79448598 ... 0.2034908 ])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "IkniiRfk4IKh",
        "outputId": "ccb4fae0-961a-4ca5-f0b9-22b87cc775fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (ipython-input-9-859760094.py, line 79)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-9-859760094.py\"\u001b[0;36m, line \u001b[0;32m79\u001b[0m\n\u001b[0;31m    print(\"Vocabulary size:\" 14)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNH2_V15C8zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, add\n",
        "from tensorflow.keras.models import Model\n",
        "import pickle\n",
        "\n",
        "# Load ResNet50 model (exclude top layer for classification)\n",
        "resnet_model = ResNet50(weights=\"imagenet\")\n",
        "feature_extractor_model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)\n",
        "\n",
        "def extract_features(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    features = feature_extractor_model.predict(img, verbose=0)\n",
        "    return features\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def create_tokenizer(captions):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(captions)\n",
        "    return tokenizer\n",
        "\n",
        "# Assume `all_captions` is a list of captions\n",
        "tokenizer = create_tokenizer(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "\n",
        "# Inputs\n",
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dense(256, activation='relu')(inputs1)\n",
        "\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = LSTM(256)(se1)\n",
        "\n",
        "# Decoder\n",
        "decoder1 = add([fe1, se2])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()\n",
        "\n",
        "# pseudo code for training loop\n",
        "# for i in range(epochs):\n",
        "#     for img, caption in zip(image_features, captions):\n",
        "#         # Convert caption to sequence of word indices\n",
        "#         # Use teacher forcing for next-word prediction\n",
        "#         model.fit([img, partial_caption_input], next_word_output)\n",
        "\n",
        "def generate_caption(model, tokenizer, photo, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo, sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = tokenizer.index_word.get(yhat)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "img_path = \"/content/LOTUS.jpeg\"\n",
        "photo = extract_features(img_path)\n",
        "caption = generate_caption(model, tokenizer, photo, max_length)\n",
        "print(\"Caption:\", caption)\n",
        "\n",
        "# print(\"Vocabulary size:\", 14)\n",
        "# print(\"Maximum caption length:\", 6)\n",
        "# print(\"Number of image features extracted:\", 2)\n",
        "\n",
        "# first_img = image_paths[0]\n",
        "# print(\"\\nFirst 10 feature values for\", first_img, \":\", image_features[first_img][0][:10])"
      ],
      "metadata": {
        "outputId": "6e7d1cd4-9cd3-4052-92da-6ec1f6aa7315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "_8PBMZ_4C9oa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_20\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_20\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_28      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_27      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_7         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │      \u001b[38;5;34m5,888\u001b[0m │ input_layer_28[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_7         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_28[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ input_layer_27[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m525,312\u001b[0m │ embedding_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │                   │            │ not_equal_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                     │                   │            │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)        │      \u001b[38;5;34m5,911\u001b[0m │ dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_28      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_27      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_7         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,888</span> │ input_layer_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_7         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ input_layer_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │                   │            │ not_equal_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                     │                   │            │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,911</span> │ dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,127,447\u001b[0m (4.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,127,447</span> (4.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,127,447\u001b[0m (4.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,127,447</span> (4.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a7a80f831a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a7a7c47efc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption: startseq person person person person person person person person person people person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLRztgs_Et9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d62438d",
        "outputId": "a8f4daa8-2010-4102-9fc3-9d21919fa661"
      },
      "source": [
        "# Placeholder for all_captions - replace with your actual caption data\n",
        "all_captions = [\n",
        "    \"startseq a person is riding a bicycle endseq\",\n",
        "    \"startseq a dog is playing in the park endseq\",\n",
        "    \"startseq a cat is sleeping on the chair endseq\",\n",
        "    \"startseq a group of people are walking on the street endseq\"\n",
        "]\n",
        "\n",
        "print(\"Placeholder all_captions list created.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Placeholder all_captions list created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7KlAGPwgPE6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p44dwiIOTYxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "26d2c07c",
        "outputId": "df6d62d5-d267-4310-c62e-74abbd120faf"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the MNIST CSV file\n",
        "mnist_df = pd.read_csv('/content/sample_data/mnist_train_small.csv', header=None)\n",
        "\n",
        "# Display the first few rows to see the data structure\n",
        "print(\"First 5 rows of the MNIST data:\")\n",
        "display(mnist_df.head())\n",
        "\n",
        "# Note: This is MNIST data (pixel values and labels), NOT an image captioning dataset.\n",
        "# The subsequent steps for image captioning will likely require significant\n",
        "# adaptation or will not be possible with this data format."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the MNIST data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
              "0    6    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "1    5    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "2    7    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "3    9    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "4    5    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "\n",
              "   779  780  781  782  783  784  \n",
              "0    0    0    0    0    0    0  \n",
              "1    0    0    0    0    0    0  \n",
              "2    0    0    0    0    0    0  \n",
              "3    0    0    0    0    0    0  \n",
              "4    0    0    0    0    0    0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d86f424e-3b3d-4d11-bbbc-9b299c6830e1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d86f424e-3b3d-4d11-bbbc-9b299c6830e1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d86f424e-3b3d-4d11-bbbc-9b299c6830e1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d86f424e-3b3d-4d11-bbbc-9b299c6830e1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dd38261e-3f7f-450f-af01-fe9248d952fa\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd38261e-3f7f-450f-af01-fe9248d952fa')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dd38261e-3f7f-450f-af01-fe9248d952fa button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8197cd10"
      },
      "source": [
        "# Task\n",
        "Train an image captioning model on the MNIST dataset provided at \"/content/sample_data/mnist_train_small.csv\" where the captions are in the format \"THE DIGIT SHOWN IS [digit]\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caeb14bd"
      },
      "source": [
        "## Load and prepare data\n",
        "\n",
        "### Subtask:\n",
        "Load the MNIST data from the CSV file and prepare it for a neural network, including separating images and labels, and reshaping the pixel data into image format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1bfaf66"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the MNIST data from the CSV file into a pandas DataFrame, separate the labels and pixel data, reshape the pixel data into 28x28 images, normalize the pixel data, and convert the labels into a NumPy array.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1111132c",
        "outputId": "1dfdf23d-4828-4149-97c6-588bedb06150"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Read the CSV file\n",
        "mnist_df = pd.read_csv('/content/sample_data/mnist_train_small.csv', header=None)\n",
        "\n",
        "# 2. Separate labels and pixel data\n",
        "mnist_labels = mnist_df.iloc[:, 0]\n",
        "mnist_pixels = mnist_df.iloc[:, 1:]\n",
        "\n",
        "# 3. Convert pixel data to NumPy array and reshape into 28x28 images\n",
        "mnist_images = mnist_pixels.values.reshape(-1, 28, 28)\n",
        "\n",
        "# 4. Normalize the pixel data\n",
        "mnist_images = mnist_images.astype('float32') / 255.0\n",
        "\n",
        "# 5. Convert labels to NumPy array\n",
        "mnist_labels = mnist_labels.values\n",
        "\n",
        "print(\"MNIST data loaded and preprocessed.\")\n",
        "print(\"Shape of images:\", mnist_images.shape)\n",
        "print(\"Shape of labels:\", mnist_labels.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST data loaded and preprocessed.\n",
            "Shape of images: (20000, 28, 28)\n",
            "Shape of labels: (20000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b19f1613"
      },
      "source": [
        "## Prepare text labels\n",
        "\n",
        "### Subtask:\n",
        "Create the corresponding text labels (e.g., \"THE DIGIT SHOWN IS zero\", \"THE DIGIT SHOWN IS one\", ...).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edd091d0"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the list of digit names and then iterate through the numerical labels to generate the corresponding text labels, storing them in a new list and printing the first few.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82a3e8d9",
        "outputId": "90ea845b-5882-4ae3-caad-09e12961682b"
      },
      "source": [
        "# 1. Create a list of digit names\n",
        "digit_names = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
        "\n",
        "# 2. Generate text labels from numerical labels\n",
        "text_labels = []\n",
        "for label in mnist_labels:\n",
        "    text_labels.append(f\"THE DIGIT SHOWN IS {digit_names[label]}\")\n",
        "\n",
        "# 3. Print the first few generated text labels\n",
        "print(\"First 5 generated text labels:\")\n",
        "print(text_labels[:5])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 generated text labels:\n",
            "['THE DIGIT SHOWN IS six', 'THE DIGIT SHOWN IS five', 'THE DIGIT SHOWN IS seven', 'THE DIGIT SHOWN IS nine', 'THE DIGIT SHOWN IS five']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6348c523"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "### Subtask:\n",
        "Create a simpler neural network model suitable for MNIST images, likely a Convolutional Neural Network (CNN), designed to output the corresponding text label.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c805a71"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary Keras layers and Model, define the CNN model architecture for MNIST, compile it, and print the summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "963085f0",
        "outputId": "3ec7e991-4635-475e-d3d5-a4d2c5f4cc8e"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Reshape images for CNN input (add channel dimension)\n",
        "mnist_images_cnn = np.expand_dims(mnist_images, axis=-1)\n",
        "\n",
        "# Convert numerical labels to one-hot encoded categorical labels\n",
        "# The vocabulary size for text labels is the number of unique digits (0-9), which is 10.\n",
        "num_classes = len(np.unique(mnist_labels))\n",
        "mnist_labels_one_hot = to_categorical(mnist_labels, num_classes=num_classes)\n",
        "\n",
        "# Define the input layer\n",
        "input_shape = (28, 28, 1)\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "# Add convolutional and pooling layers\n",
        "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_layer)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(pool1)\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "# Flatten the output\n",
        "flatten = Flatten()(pool2)\n",
        "\n",
        "# Add dense layers\n",
        "dense1 = Dense(128, activation='relu')(flatten)\n",
        "\n",
        "# Add the final dense layer with softmax activation for text label vocabulary\n",
        "# The size of the output layer should match the number of unique text labels (10 digits)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense1)\n",
        "\n",
        "# Create the model\n",
        "mnist_cnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "mnist_cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "mnist_cnn_model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_21\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_21\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_29 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59bbe336"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Train the model on the prepared MNIST image data and the corresponding text labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feef3c7f"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the CNN model using the prepared image data and one-hot encoded labels, including a validation split and storing the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31ff6e8d",
        "outputId": "33e0b739-0f6c-4112-f171-3bc3604f29cd"
      },
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "history = mnist_cnn_model.fit(\n",
        "    mnist_images_cnn,\n",
        "    mnist_labels_one_hot,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2 # Use 20% of the data for validation\n",
        ")\n",
        "\n",
        "print(\"Model training completed.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 62ms/step - accuracy: 0.7695 - loss: 0.7938 - val_accuracy: 0.9622 - val_loss: 0.1293\n",
            "Epoch 2/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - accuracy: 0.9689 - loss: 0.0964 - val_accuracy: 0.9725 - val_loss: 0.0847\n",
            "Epoch 3/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 61ms/step - accuracy: 0.9806 - loss: 0.0595 - val_accuracy: 0.9783 - val_loss: 0.0634\n",
            "Epoch 4/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 68ms/step - accuracy: 0.9882 - loss: 0.0369 - val_accuracy: 0.9812 - val_loss: 0.0586\n",
            "Epoch 5/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 61ms/step - accuracy: 0.9914 - loss: 0.0286 - val_accuracy: 0.9837 - val_loss: 0.0586\n",
            "Epoch 6/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 62ms/step - accuracy: 0.9922 - loss: 0.0246 - val_accuracy: 0.9815 - val_loss: 0.0554\n",
            "Epoch 7/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - accuracy: 0.9955 - loss: 0.0151 - val_accuracy: 0.9852 - val_loss: 0.0514\n",
            "Epoch 8/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 66ms/step - accuracy: 0.9969 - loss: 0.0102 - val_accuracy: 0.9840 - val_loss: 0.0536\n",
            "Epoch 9/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - accuracy: 0.9966 - loss: 0.0106 - val_accuracy: 0.9850 - val_loss: 0.0543\n",
            "Epoch 10/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 60ms/step - accuracy: 0.9978 - loss: 0.0081 - val_accuracy: 0.9845 - val_loss: 0.0531\n",
            "Model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53069c6d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to train a linear regression model to predict car prices. First, I need to load the data and prepare it for modeling by selecting relevant features and splitting it into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "6a3ae081",
        "outputId": "2e2d9c69-714c-4acf-b846-f814c5c6c5ad"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('car_prices.csv')\n",
        "\n",
        "# Select features and target variable\n",
        "features = ['year', 'odometer', 'condition']\n",
        "target = 'sellingprice'\n",
        "\n",
        "# Handle potential missing values in selected columns (simple imputation with mean)\n",
        "# A more sophisticated approach might be needed depending on the data\n",
        "for col in features + [target]:\n",
        "    if df[col].isnull().any():\n",
        "        if df[col].dtype in [np.float64, np.int64]:\n",
        "            df[col].fillna(df[col].mean(), inplace=True)\n",
        "        else:\n",
        "            # For non-numeric columns, a different imputation strategy might be needed\n",
        "            # For simplicity, we'll fill with a placeholder or mode if appropriate\n",
        "            # For 'condition', which is likely categorical, we'll fill with the mode\n",
        "            if col == 'condition':\n",
        "                 df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "            else:\n",
        "                 # For other non-numeric columns, dropping or a different approach might be better\n",
        "                 # For this task, we'll assume the selected features are numeric or can be handled\n",
        "                 pass # Or implement other strategies as needed\n",
        "\n",
        "# Convert 'condition' to numerical if it's not already (e.g., one-hot encoding or label encoding)\n",
        "# Assuming 'condition' is a string, we'll use one-hot encoding\n",
        "if df['condition'].dtype == 'object':\n",
        "    df = pd.get_dummies(df, columns=['condition'], drop_first=True)\n",
        "    # Update features list to include the new one-hot encoded columns\n",
        "    features = ['year', 'odometer'] + [col for col in df.columns if col.startswith('condition_')]\n",
        "\n",
        "# Prepare data for modeling\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the coefficients and intercept\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "print(\"Model Intercept:\", model.intercept_)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'car_prices.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-4148992784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'car_prices.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Select features and target variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'car_prices.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33589e36"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the file 'car_prices.csv' was not found. I need to use the correct path to load the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "QPwRc6OBSfHB",
        "outputId": "9523912d-171a-4bef-ded3-6c85b69c22e2"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset with the correct path\n",
        "df = pd.read_csv('/data/car_prices.csv')\n",
        "\n",
        "# Select features and target variable\n",
        "features = ['year', 'odometer', 'condition']\n",
        "target = 'sellingprice'\n",
        "\n",
        "# Handle potential missing values in selected columns (simple imputation with mean)\n",
        "# A more sophisticated approach might be needed depending on the data\n",
        "for col in features + [target]:\n",
        "    if col in df.columns and df[col].isnull().any():\n",
        "        if df[col].dtype in [np.float64, np.int64]:\n",
        "            df[col].fillna(df[col].mean(), inplace=True)\n",
        "        else:\n",
        "            # For non-numeric columns, a different imputation strategy might be needed\n",
        "            # For simplicity, we'll fill with a placeholder or mode if appropriate\n",
        "            # For 'condition', which is likely categorical, we'll fill with the mode\n",
        "            if col == 'condition':\n",
        "                 df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "            else:\n",
        "                 # For other non-numeric columns, dropping or a different approach might be better\n",
        "                 # For this task, we'll assume the selected features are numeric or can be handled\n",
        "                 pass # Or implement other strategies as needed\n",
        "\n",
        "\n",
        "# Convert 'condition' to numerical if it's not already (e.g., one-hot encoding or label encoding)\n",
        "# Assuming 'condition' is a string, we'll use one-hot encoding\n",
        "if 'condition' in df.columns and df['condition'].dtype == 'object':\n",
        "    df = pd.get_dummies(df, columns=['condition'], prefix='condition', dummy_na=False)\n",
        "    # Update features list to include the new one-hot encoded columns, ensuring they exist in the dataframe\n",
        "    condition_cols = [col for col in df.columns if col.startswith('condition_')]\n",
        "    features = ['year', 'odometer'] + condition_cols\n",
        "elif 'condition' not in df.columns:\n",
        "    features = ['year', 'odometer'] # Proceed without 'condition' if it's not in the dataframe\n",
        "\n",
        "# Ensure all selected features exist in the dataframe before proceeding\n",
        "features = [f for f in features if f in df.columns]\n",
        "\n",
        "# Prepare data for modeling\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the coefficients and intercept\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "print(\"Model Intercept:\", model.intercept_)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/data/car_prices.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-25-575080421.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the dataset with the correct path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/car_prices.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Select features and target variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/car_prices.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "085777d2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed again because the file path was incorrect. I need to try the path '/home/jupyter/data/car_prices.csv' which is a common location for data files in this environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "J3ybswmaSiF3",
        "outputId": "08c2732c-5ad1-42fb-d465-885574368144"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset with the correct path\n",
        "try:\n",
        "    df = pd.read_csv('/home/jupyter/data/car_prices.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found at /home/jupyter/data/car_prices.csv\")\n",
        "    # If the file is not found here, try another common location or indicate failure\n",
        "    # For this attempt, we'll assume this path is correct and the previous errors were typos\n",
        "    # If this still fails, I would need to investigate the file location further.\n",
        "\n",
        "\n",
        "# Select features and target variable\n",
        "features = ['year', 'odometer', 'condition']\n",
        "target = 'sellingprice'\n",
        "\n",
        "# Handle potential missing values in selected columns (simple imputation with mean)\n",
        "# A more sophisticated approach might be needed depending on the data\n",
        "for col in features + [target]:\n",
        "    if col in df.columns and df[col].isnull().any():\n",
        "        if df[col].dtype in [np.float64, np.int64]:\n",
        "            df[col].fillna(df[col].mean(), inplace=True)\n",
        "        else:\n",
        "            # For non-numeric columns, a different imputation strategy might be needed\n",
        "            # For simplicity, we'll fill with a placeholder or mode if appropriate\n",
        "            # For 'condition', which is likely categorical, we'll fill with the mode\n",
        "            if col == 'condition':\n",
        "                 df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "            else:\n",
        "                 # For other non-numeric columns, dropping or a different approach might be better\n",
        "                 # For this task, we'll assume the selected features are numeric or can be handled\n",
        "                 pass # Or implement other strategies as needed\n",
        "\n",
        "\n",
        "# Convert 'condition' to numerical if it's not already (e.g., one-hot encoding or label encoding)\n",
        "# Assuming 'condition' is a string, we'll use one-hot encoding\n",
        "if 'condition' in df.columns and df['condition'].dtype == 'object':\n",
        "    df = pd.get_dummies(df, columns=['condition'], prefix='condition', dummy_na=False)\n",
        "    # Update features list to include the new one-hot encoded columns, ensuring they exist in the dataframe\n",
        "    condition_cols = [col for col in df.columns if col.startswith('condition_')]\n",
        "    features = ['year', 'odometer'] + condition_cols\n",
        "elif 'condition' not in df.columns:\n",
        "    features = ['year', 'odometer'] # Proceed without 'condition' if it's not in the dataframe\n",
        "\n",
        "# Ensure all selected features exist in the dataframe before proceeding\n",
        "features = [f for f in features if f in df.columns]\n",
        "\n",
        "# Prepare data for modeling\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the coefficients and intercept\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "print(\"Model Intercept:\", model.intercept_)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found at /home/jupyter/data/car_prices.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-26-4120002824.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# A more sophisticated approach might be needed depending on the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4db7be73"
      },
      "source": [
        "## Load and prepare data\n",
        "\n",
        "### Subtask:\n",
        "Load the MNIST data from the CSV file and prepare it for a neural network, including separating images and labels, and reshaping the pixel data into image format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd03550f"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the MNIST data from the CSV file into a pandas DataFrame, separate the labels and pixel data, reshape the pixel data into 28x28 images, normalize the pixel data, and convert the labels into a NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8273f15",
        "outputId": "169638a8-a5b6-4b7f-ffc4-a7f12a01fc9e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Read the CSV file\n",
        "mnist_df = pd.read_csv('/content/sample_data/mnist_train_small.csv', header=None)\n",
        "\n",
        "# 2. Separate labels and pixel data\n",
        "mnist_labels = mnist_df.iloc[:, 0]\n",
        "mnist_pixels = mnist_df.iloc[:, 1:]\n",
        "\n",
        "# 3. Convert pixel data to NumPy array and reshape into 28x28 images\n",
        "mnist_images = mnist_pixels.values.reshape(-1, 28, 28)\n",
        "\n",
        "# 4. Normalize the pixel data\n",
        "mnist_images = mnist_images.astype('float32') / 255.0\n",
        "\n",
        "# 5. Convert labels to NumPy array\n",
        "mnist_labels = mnist_labels.values\n",
        "\n",
        "print(\"MNIST data loaded and preprocessed.\")\n",
        "print(\"Shape of images:\", mnist_images.shape)\n",
        "print(\"Shape of labels:\", mnist_labels.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST data loaded and preprocessed.\n",
            "Shape of images: (20000, 28, 28)\n",
            "Shape of labels: (20000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61e663cd"
      },
      "source": [
        "## Prepare text labels\n",
        "\n",
        "### Subtask:\n",
        "Create the corresponding text labels (e.g., \"THE DIGIT SHOWN IS zero\", \"THE DIGIT SHOWN IS one\", ...)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "725f6fff"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the list of digit names and then iterate through the numerical labels to generate the corresponding text labels, storing them in a new list and printing the first few."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c1dc754",
        "outputId": "5a8d7a6b-bf2c-4a51-ff0e-be7a2f23a453"
      },
      "source": [
        "# 1. Create a list of digit names\n",
        "digit_names = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
        "\n",
        "# 2. Generate text labels from numerical labels\n",
        "text_labels = []\n",
        "for label in mnist_labels:\n",
        "    text_labels.append(f\"THE DIGIT SHOWN IS {digit_names[label]}\")\n",
        "\n",
        "# 3. Print the first few generated text labels\n",
        "print(\"First 5 generated text labels:\")\n",
        "print(text_labels[:5])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 generated text labels:\n",
            "['THE DIGIT SHOWN IS six', 'THE DIGIT SHOWN IS five', 'THE DIGIT SHOWN IS seven', 'THE DIGIT SHOWN IS nine', 'THE DIGIT SHOWN IS five']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d740e4b"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "### Subtask:\n",
        "Create a simpler neural network model suitable for MNIST images, likely a Convolutional Neural Network (CNN), designed to output the corresponding text label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e62264d1"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary Keras layers and Model, define the CNN model architecture for MNIST, compile it, and print the summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "f641781b",
        "outputId": "7106f3a8-f612-4d6a-e6df-ac4979082e61"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Reshape images for CNN input (add channel dimension)\n",
        "mnist_images_cnn = np.expand_dims(mnist_images, axis=-1)\n",
        "\n",
        "# Convert numerical labels to one-hot encoded categorical labels\n",
        "# The vocabulary size for text labels is the number of unique digits (0-9), which is 10.\n",
        "num_classes = len(np.unique(mnist_labels))\n",
        "mnist_labels_one_hot = to_categorical(mnist_labels, num_classes=num_classes)\n",
        "\n",
        "# Define the input layer\n",
        "input_shape = (28, 28, 1)\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "# Add convolutional and pooling layers\n",
        "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_layer)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(pool1)\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "# Flatten the output\n",
        "flatten = Flatten()(pool2)\n",
        "\n",
        "# Add dense layers\n",
        "dense1 = Dense(128, activation='relu')(flatten)\n",
        "\n",
        "# Add the final dense layer with softmax activation for text label vocabulary\n",
        "# The size of the output layer should match the number of unique text labels (10 digits)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense1)\n",
        "\n",
        "# Create the model\n",
        "mnist_cnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "mnist_cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "mnist_cnn_model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_22\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_22\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_30 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb5448d6"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Train the model on the prepared MNIST image data and the corresponding text labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "236dd3da"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the CNN model using the prepared image data and one-hot encoded labels, including a validation split and storing the training history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65bdc0ca",
        "outputId": "ee4dfe19-a453-4818-cbcd-2d690d51433e"
      },
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "history = mnist_cnn_model.fit(\n",
        "    mnist_images_cnn,\n",
        "    mnist_labels_one_hot,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2 # Use 20% of the data for validation\n",
        ")\n",
        "\n",
        "print(\"Model training completed.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 66ms/step - accuracy: 0.7774 - loss: 0.7448 - val_accuracy: 0.9613 - val_loss: 0.1290\n",
            "Epoch 2/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 63ms/step - accuracy: 0.9695 - loss: 0.0944 - val_accuracy: 0.9697 - val_loss: 0.0891\n",
            "Epoch 3/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - accuracy: 0.9801 - loss: 0.0646 - val_accuracy: 0.9783 - val_loss: 0.0697\n",
            "Epoch 4/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 71ms/step - accuracy: 0.9850 - loss: 0.0453 - val_accuracy: 0.9830 - val_loss: 0.0641\n",
            "Epoch 5/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 73ms/step - accuracy: 0.9903 - loss: 0.0317 - val_accuracy: 0.9822 - val_loss: 0.0564\n",
            "Epoch 6/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 62ms/step - accuracy: 0.9934 - loss: 0.0225 - val_accuracy: 0.9835 - val_loss: 0.0510\n",
            "Epoch 7/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 68ms/step - accuracy: 0.9943 - loss: 0.0173 - val_accuracy: 0.9843 - val_loss: 0.0534\n",
            "Epoch 8/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 61ms/step - accuracy: 0.9956 - loss: 0.0153 - val_accuracy: 0.9865 - val_loss: 0.0521\n",
            "Epoch 9/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 62ms/step - accuracy: 0.9963 - loss: 0.0117 - val_accuracy: 0.9855 - val_loss: 0.0518\n",
            "Epoch 10/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - accuracy: 0.9975 - loss: 0.0082 - val_accuracy: 0.9852 - val_loss: 0.0577\n",
            "Model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14c13d00"
      },
      "source": [
        "## Generate text labels\n",
        "\n",
        "### Subtask:\n",
        "Use the trained model to predict the digit from an image and generate the corresponding text label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83993407"
      },
      "source": [
        "**Reasoning**:\n",
        "Select a sample image from the dataset, use the trained model to predict the digit, get the predicted digit's name, and format the output as the desired text label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "ced20e5d",
        "outputId": "de24ae07-36e3-4e1d-add1-89e78f06d41c"
      },
      "source": [
        "# Select a sample image (e.g., the first image from the dataset)\n",
        "sample_image = mnist_images_cnn[0]\n",
        "sample_label = mnist_labels[0]\n",
        "\n",
        "# Reshape the sample image to match the model's input shape (add batch dimension)\n",
        "sample_image_input = np.expand_dims(sample_image, axis=0)\n",
        "\n",
        "# Use the trained model to predict the digit\n",
        "predicted_probabilities = mnist_cnn_model.predict(sample_image_input)\n",
        "predicted_digit = np.argmax(predicted_probabilities)\n",
        "\n",
        "# Get the predicted digit's name\n",
        "predicted_digit_name = digit_names[predicted_digit]\n",
        "\n",
        "# Generate the text label\n",
        "generated_text_label = f\"THE DIGIT SHOWN IS {predicted_digit_name}\"\n",
        "\n",
        "# Print the original label and the generated text label\n",
        "print(f\"Original Digit Label: {sample_label}\")\n",
        "print(f\"Predicted Digit: {predicted_digit}\")\n",
        "print(f\"Generated Text Label: {generated_text_label}\")\n",
        "\n",
        "# Optional: Display the sample image\n",
        "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
        "plt.title(f\"Predicted: {predicted_digit_name}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
            "Original Digit Label: 6\n",
            "Predicted Digit: 6\n",
            "Generated Text Label: THE DIGIT SHOWN IS six\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d771486a",
        "outputId": "c992f454-13a9-436c-bef1-426ddce09da9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# --- Step 1: Load and prepare data ---\n",
        "print(\"--- Step 1: Loading and preparing data ---\")\n",
        "# 1. Read the CSV file\n",
        "mnist_df = pd.read_csv('/content/sample_data/mnist_train_small.csv', header=None)\n",
        "\n",
        "# 2. Separate labels and pixel data\n",
        "mnist_labels = mnist_df.iloc[:, 0]\n",
        "mnist_pixels = mnist_df.iloc[:, 1:]\n",
        "\n",
        "# 3. Convert pixel data to NumPy array and reshape into 28x28 images\n",
        "mnist_images = mnist_pixels.values.reshape(-1, 28, 28)\n",
        "\n",
        "# 4. Normalize the pixel data\n",
        "mnist_images = mnist_images.astype('float32') / 255.0\n",
        "\n",
        "# 5. Convert labels to NumPy array\n",
        "mnist_labels = mnist_labels.values\n",
        "\n",
        "print(\"MNIST data loaded and preprocessed.\")\n",
        "print(\"Shape of images:\", mnist_images.shape)\n",
        "print(\"Shape of labels:\", mnist_labels.shape)\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- Step 2: Prepare text labels ---\n",
        "print(\"--- Step 2: Preparing text labels ---\")\n",
        "# 1. Create a list of digit names\n",
        "digit_names = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
        "\n",
        "# 2. Generate text labels from numerical labels\n",
        "text_labels = []\n",
        "for label in mnist_labels:\n",
        "    text_labels.append(f\"THE DIGIT SHOWN IS {digit_names[label]}\")\n",
        "\n",
        "# 3. Print the first few generated text labels\n",
        "print(\"First 5 generated text labels:\")\n",
        "print(text_labels[:5])\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- Step 3: Build a model ---\n",
        "print(\"--- Step 3: Building the model ---\")\n",
        "# Reshape images for CNN input (add channel dimension)\n",
        "mnist_images_cnn = np.expand_dims(mnist_images, axis=-1)\n",
        "\n",
        "# Convert numerical labels to one-hot encoded categorical labels\n",
        "num_classes = len(np.unique(mnist_labels))\n",
        "mnist_labels_one_hot = to_categorical(mnist_labels, num_classes=num_classes)\n",
        "\n",
        "# Define the input layer\n",
        "input_shape = (28, 28, 1)\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "# Add convolutional and pooling layers\n",
        "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_layer)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(pool1)\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "# Flatten the output\n",
        "flatten = Flatten()(pool2)\n",
        "\n",
        "# Add dense layers\n",
        "dense1 = Dense(128, activation='relu')(flatten)\n",
        "\n",
        "# Add the final dense layer with softmax activation for text label vocabulary\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense1)\n",
        "\n",
        "# Create the model\n",
        "mnist_cnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "mnist_cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model built.\")\n",
        "mnist_cnn_model.summary()\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- Step 4: Train the model ---\n",
        "print(\"--- Step 4: Training the model ---\")\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "history = mnist_cnn_model.fit(\n",
        "    mnist_images_cnn,\n",
        "    mnist_labels_one_hot,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2 # Use 20% of the data for validation\n",
        ")\n",
        "\n",
        "print(\"Model training completed.\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- Step 5: Generate text labels ---\n",
        "print(\"--- Step 5: Generating text labels for a sample image ---\")\n",
        "# Select a sample image (e.g., the first image from the dataset)\n",
        "sample_image = mnist_images_cnn[0]\n",
        "sample_label = mnist_labels[0]\n",
        "\n",
        "# Reshape the sample image to match the model's input shape (add batch dimension)\n",
        "sample_image_input = np.expand_dims(sample_image, axis=0)\n",
        "\n",
        "# Use the trained model to predict the digit\n",
        "predicted_probabilities = mnist_cnn_model.predict(sample_image_input)\n",
        "predicted_digit = np.argmax(predicted_probabilities)\n",
        "\n",
        "# Get the predicted digit's name\n",
        "predicted_digit_name = digit_names[predicted_digit]\n",
        "\n",
        "# Generate the text label\n",
        "generated_text_label = f\"THE DIGIT SHOWN IS {predicted_digit_name}\"\n",
        "\n",
        "# Print the original label and the generated text label\n",
        "print(f\"Original Digit Label: {sample_label}\")\n",
        "print(f\"Predicted Digit: {predicted_digit}\")\n",
        "print(f\"Generated Text Label: {generated_text_label}\")\n",
        "\n",
        "# Optional: Display the sample image\n",
        "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
        "plt.title(f\"Predicted: {predicted_digit_name}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print(\"-\" * 40)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Loading and preparing data ---\n",
            "MNIST data loaded and preprocessed.\n",
            "Shape of images: (20000, 28, 28)\n",
            "Shape of labels: (20000,)\n",
            "----------------------------------------\n",
            "--- Step 2: Preparing text labels ---\n",
            "First 5 generated text labels:\n",
            "['THE DIGIT SHOWN IS six', 'THE DIGIT SHOWN IS five', 'THE DIGIT SHOWN IS seven', 'THE DIGIT SHOWN IS nine', 'THE DIGIT SHOWN IS five']\n",
            "----------------------------------------\n",
            "--- Step 3: Building the model ---\n",
            "Model built.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_23\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_23\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_31 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "--- Step 4: Training the model ---\n",
            "Epoch 1/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 63ms/step - accuracy: 0.7653 - loss: 0.7692 - val_accuracy: 0.9572 - val_loss: 0.1353\n",
            "Epoch 2/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.9686 - loss: 0.1033 - val_accuracy: 0.9745 - val_loss: 0.0814\n",
            "Epoch 3/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 69ms/step - accuracy: 0.9811 - loss: 0.0634 - val_accuracy: 0.9730 - val_loss: 0.0806\n",
            "Epoch 4/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 62ms/step - accuracy: 0.9841 - loss: 0.0511 - val_accuracy: 0.9793 - val_loss: 0.0578\n",
            "Epoch 5/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 64ms/step - accuracy: 0.9900 - loss: 0.0308 - val_accuracy: 0.9808 - val_loss: 0.0522\n",
            "Epoch 6/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - accuracy: 0.9922 - loss: 0.0244 - val_accuracy: 0.9833 - val_loss: 0.0548\n",
            "Epoch 7/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 65ms/step - accuracy: 0.9935 - loss: 0.0193 - val_accuracy: 0.9862 - val_loss: 0.0472\n",
            "Epoch 8/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 63ms/step - accuracy: 0.9953 - loss: 0.0136 - val_accuracy: 0.9862 - val_loss: 0.0538\n",
            "Epoch 9/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 71ms/step - accuracy: 0.9966 - loss: 0.0117 - val_accuracy: 0.9862 - val_loss: 0.0468\n",
            "Epoch 10/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.9979 - loss: 0.0074 - val_accuracy: 0.9855 - val_loss: 0.0553\n",
            "Model training completed.\n",
            "----------------------------------------\n",
            "--- Step 5: Generating text labels for a sample image ---\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "Original Digit Label: 6\n",
            "Predicted Digit: 6\n",
            "Generated Text Label: THE DIGIT SHOWN IS six\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Snippets: Importing libraries",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}